---

# The cloud provider
# (also embedded in the the file names of the tasks we'll run)
provider: openstack

# A string prepended to each cloud object we create
# (e.g. the data volume and compute instances)
instance_base_name: fragmentor
# The network for the instances
instance_network: xchem-private

# Cluster node selection
#
# OpenStack   AWS
# Type        Type          cores mem disk
# ---------   -----------   ----- --- ----
# c1.medium                     2   4   40
# c1.large                      2   8   80
# c1.xlarge                     4  16  100
# c1.3xl                       16  48  200
# c1.4xl                       28  90  400
# c1.xxlarge  t3.2xlarge        8  32  160
# c2.medium                     4   4   40
# c2.xlarge                    16  16  160
# c3.medium                     4   8   40
# c3.large                      8  16   80
# c3.xlarge                    16  32  160
# l1.6xl                       60 360  750

head_type: c1.xxlarge
head_image_name: ScientificLinux-7-NoGui
head_addr: 130.246.215.45

worker_type: c1.xxlarge
worker_image_name: ScientificLinux-7-NoGui
worker_count: 40

# The size of the 'shared' volume (G).
# This is attached to the head not and exposed
# as an NFS-share to the worker nodes in the cluster.
volume_size_g: 8000
# Format the volume (ext4) prior to the first mount?
volume_initialise: yes
# Delete the volume when the cluster is deleted?
volume_delete: yes

# Application control.
#
# At the moment slurm needs MUNGE, installing slurm expects you
# to install MUNGE. You cannot set install_munge to 'no'
# and install_slurm to 'yes'. You can install MUNGE without slurm.
install_nextflow: yes
install_pulsar: yes
install_munge: yes
install_slurm: yes

# Redis (for deduplication experiments)
install_redis: no
# Memcached (for deduplication experiments)
install_memcached: no
