---

# The cloud provider
# (also embedded in the the file names of the tasks we'll run)
provider: openstack

# A string prepended to each cloud object we create
# (e.g. the data volume and compute instances)
instance_base_name: graph

# Cluster node selection
#
# OpenStack
# Type           cores mem disk
# ---------      ----- --- ----
# c1.medium          2   4   40
# c1.xlarge          4  16  100
# c1.4xl            28  90  400
# c1.xxlarge         8  32  160
# c2.medium          4   4   40
# c2.xlarge         16  16  160
# c3.medium          4   8   40
# c3.large           8  16   80
# c3.xlarge         16  32  160

# The following is a total allocation of: -
#
#   104 cores
#   192 Gi RAM
# 1,040 Gi disk

head_type: c2.xlarge
head_image_name: ScientificLinux-7-NoGui
head_addr: 130.246.215.45

worker_type: c3.large
worker_image_name: ScientificLinux-7-NoGui
worker_count: 11

# The size of the 'shared' volume (G).
# This is attached to the head not and exposed
# as an NFS-share to the worker nodes in the cluster.
volume_size_g: 3000

# Application control.
#
# At the moment slurm needs MUNGE, installing slurm expects you
# to install MUNGE. You cannot set install_munge to 'no'
# and install_slurm to 'yes'. You can install MUNGE without slurm.
install_nextflow: yes
install_pulsar: yes
install_munge: yes
install_slurm: yes
