---

# The cloud provider
# (also embedded in the the file names of the tasks we'll run)
provider: openstack

# A string prepended to each cloud object we create
# (e.g. the data volume and compute instances)
instance_base_name: cluster

# Cluster node selection
#
# OpenStack
# Type           cores mem disk
# ---------      ----- --- ----
# c1.medium          2   4   40
# c1.xlarge          4  16  100
# c1.4xl            28  90  400
# c1.xxlarge         8  32  160
# c2.medium          4   4   40
# c2.xlarge         16  16  160
# c3.medium          4   8   40
# c3.large           8  16   80
# c3.xlarge         16  32  160

# The following amounts to a total allocation of: -
#
#    96 cores
#   180 Gi RAM
#   960 Gi disk

head_type: c2.xlarge
head_image_name: ScientificLinux-7-NoGui
head_addr: 130.246.215.45

worker_type: c3.large
worker_image_name: ScientificLinux-7-NoGui
worker_count: 10

# The size of the 'shared' volume (G).
# This is attached to the head not and exposed
# as an NFS-share to the worker nodes in the cluster.
volume_size_g: 3000
# Format the volume (ext4) prior to the first mount?
volume_initialise: yes
# Delete the volume when the cluster is deleted?
volume_delete: yes

# Application control.
#
# At the moment slurm needs MUNGE, installing slurm expects you
# to install MUNGE. You cannot set install_munge to 'no'
# and install_slurm to 'yes'. You can install MUNGE without slurm.
install_nextflow: yes
install_pulsar: yes
install_munge: yes
install_slurm: yes
