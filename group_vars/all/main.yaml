---

# The cloud provider
# (also embedded in the the file names of the tasks we'll run)
provider: openstack

# A string prepended to each cloud object we create
# (e.g. the data volume and compute instances)
instance_base_name: cluster
# The network for the instances
instance_network: xchem-private

# Cluster node selection
#
# OpenStack
# Type           cores mem disk
# ---------      ----- --- ----
# c1.medium          2   4   40
# c1.large           2   8   80
# c1.xlarge          4  16  100
# c1.3xl            16  48  200
# c1.4xl            28  90  400
# c1.xxlarge         8  32  160
# c2.medium          4   4   40
# c2.xlarge         16  16  160
# c3.medium          4   8   40
# c3.large           8  16   80
# c3.xlarge         16  32  160

# The following amounts to a total allocation of: -
#
#   272 cores
#   560 Gi RAM
#  2760 Gi disk

head_type: c1.3xl
head_image_name: ScientificLinux-7-NoGui
head_addr: 130.246.215.45

worker_type: c3.xlarge
worker_image_name: ScientificLinux-7-NoGui
worker_count: 16

# The size of the 'shared' volume (G).
# This is attached to the head not and exposed
# as an NFS-share to the worker nodes in the cluster.
volume_size_g: 4000
# Format the volume (ext4) prior to the first mount?
volume_initialise: yes
# Delete the volume when the cluster is deleted?
volume_delete: yes

# Application control.
#
# At the moment slurm needs MUNGE, installing slurm expects you
# to install MUNGE. You cannot set install_munge to 'no'
# and install_slurm to 'yes'. You can install MUNGE without slurm.
install_nextflow: yes
install_pulsar: yes
install_munge: yes
install_slurm: yes
